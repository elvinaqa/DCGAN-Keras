# -*- coding: utf-8 -*-
"""DCGAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mQSM6iwOpjtYjUvM09Eh9fHWLJqXdPxU

Better suited GANs for image data
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import seaborn as sns

from tensorflow.keras.datasets import mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()

plt.imshow(X_train[10])

X_train.min()

X_train = X_train/255
X_train = X_train.reshape(-1, 28, 28, 1) * 2. -1.

X_train.min()
# tanh used thus made values to be -1 to 1 range

X_train.max()

zeros = X_train[y_train==0]

zeros.shape

from tensorflow.keras.layers import Dense, Reshape, Dropout, LeakyReLU, Flatten, BatchNormalization, Conv2D, Conv2DTranspose
from tensorflow.keras.models import  Sequential

np.random.seed(42)
tf.random.set_seed(42)

coding_size = 100

7*7*128

generator = Sequential()
generator.add(Dense(7*7*128, input_shape=[coding_size]))
generator.add(Reshape([7, 7, 128]))
generator.add(BatchNormalization())
generator.add(Conv2DTranspose(64, kernel_size=5, strides=2, padding='same', activation='relu'))
generator.add(BatchNormalization())
generator.add(Conv2DTranspose(1, kernel_size=5, strides=2, padding='same', activation='tanh'))

discriminator = Sequential()
discriminator.add(Conv2D(64, kernel_size=5, strides=2, padding='same', activation=LeakyReLU(0.3), input_shape=[28, 28, 1]))
discriminator.add(Dropout(0.5))
discriminator.add(Conv2D(128, kernel_size=5, strides=2, padding='same', activation=LeakyReLU(0.3)))
discriminator.add(Dropout(0.5))
discriminator.add(Flatten())
discriminator.add(Dense(1, activation='sigmoid'))

GAN = Sequential([generator, discriminator])

discriminator.compile(loss='binary_crossentropy', optimizer='adam')
discriminator.trainable = False

GAN.compile(loss='binary_crossentropy', optimizer='adam')

GAN.layers[1].layers

GAN.layers[1].summary()

GAN.layers[0].summary()

GAN.summary()

batch_size = 32

data = zeros

dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(buffer_size=1000)

type(dataset)

dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)

epochs = 20

generator, discriminator = GAN.layers
for epoch in range(epochs):
  print(f"On epoch {epoch+1}")
  i = 0
  for X_batch in dataset:
    i = i+1
    if i%20 == 0:
      print(f"\t Batch number {i} of {len(data//batch_size)}")
    
    # TRAIN DISCRIMINATOR
    noise = tf.random.normal(shape=[batch_size, coding_size])

    # Generate Numbers on NOISE
    gen_images = generator(noise)

    # Concat generated ones with Fake
    X_real_vs_fake = tf.concat([gen_images, tf.dtypes.cast(X_batch, tf.float32)], axis=0)

    # Y set 0 for fake, 1 for real imgs
    y1 = tf.constant([[0.0]]*batch_size + [[1.0]]*batch_size)
    discriminator.trainable = True

    discriminator.train_on_batch(X_real_vs_fake, y1)


    # TRAIN GENERATOR
    noise = tf.random.normal(shape=[batch_size, coding_size])
    # Discriminator are made to believe generated images are real 1.0
    y2 = tf.constant([[1.0]]*batch_size)

    discriminator.trainable = False

    GAN.train_on_batch(noise, y2)

noise = tf.random.normal(shape=[10, coding_size])
noise.shape

plt.imshow(noise)

images = generator(noise)

an_image = images[0]

for image in images:
  plt.imshow(image.numpy().reshape(28, 28))
  plt.show()

# Model Collapse happens ? Diversity?

